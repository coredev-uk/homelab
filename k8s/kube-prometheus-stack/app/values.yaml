# Global settings
fullnameOverride: ""

# Crds
crds:
  enabled: true
  ## The CRD upgrade job mitigates the limitation of helm not being able to upgrade CRDs.
  ## The job will apply the CRDs to the cluster before the operator is deployed, using helm hooks.
  ## It deploys a corresponding clusterrole, clusterrolebinding and serviceaccount to apply the CRDs.
  ## This feature is in preview, off by default and may change in the future.
  upgradeJob:
    enabled: true
    forceConflicts: false
    image:
      busybox:
        registry: docker.io
        repository: busybox
        tag: "latest"
        sha: ""
        pullPolicy: IfNotPresent
      kubectl:
        registry: registry.k8s.io
        repository: kubectl
        tag: ""  # defaults to the Kubernetes version
        sha: ""
        pullPolicy: IfNotPresent

# Prometheus Operator
prometheusOperator:
  enabled: true

# Prometheus configuration
prometheus:
  enabled: true
  prometheusSpec:
    retention: 7d
    retentionSize: "18GB"
    # Increase scrape interval to reduce load (default was 30s)
    scrapeInterval: 60s
    evaluationInterval: 60s
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: longhorn
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 20Gi
    resources:
      requests:
        memory: "1Gi"
        cpu: "200m"
      limits:
        memory: "3Gi"
        cpu: "1000m"
    externalLabels:
      cluster: 'homelab'
      replica: 'prometheus-1'
    additionalScrapeConfigs:
      - job_name: 'pihole-exporter'
        static_configs:
          - targets: ['pihole-exporter.pihole.svc.cluster.local:9617']
      - job_name: 'frigate-exporter'
        static_configs:
          - targets: ['frigate-exporter.frigate.svc.cluster.local:9999']
      - job_name: 'qbittorrent-vpn-exporter'
        static_configs:
          - targets: ['qbittorrent-vpn-exporter.qbittorrent.svc.cluster.local:9090']
      - job_name: 'sabnzbd-vpn-exporter'
        static_configs:
          - targets: ['sabnzbd-vpn-exporter.sabnzbd.svc.cluster.local:9090']
      # Longhorn with metric filtering to reduce high-cardinality histograms
      - job_name: 'longhorn'
        static_configs:
          - targets: ['longhorn-backend.longhorn-system.svc.cluster.local:9500']
        metric_relabel_configs:
          # Drop high-cardinality REST client histogram buckets
          - source_labels: [__name__]
            regex: 'longhorn_rest_client_(rate_limiter_latency|request_latency)_seconds_bucket'
            action: drop
  
  # Prometheus ingress
  ingress:
    enabled: true
    ingressClassName: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: web,websecure
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/redirect-to-https: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
      traefik.ingress.kubernetes.io/router.middlewares: pocket-id-tinyauth-forwardauth@kubernetescrd
    hosts:
      - prometheus.home.coredev.uk
    paths:
      - /
    pathType: Prefix
    tls:
      - secretName: prometheus-tls
        hosts:
          - prometheus.home.coredev.uk
  
  # Add Glance annotations to Prometheus service
  service:
    annotations:
      glance/name: "Prometheus"
      glance/icon: "di:prometheus"
      glance/url: "https://prometheus.home.coredev.uk"
      glance/description: "Metrics Collection"

# Grafana configuration
grafana:
  enabled: true
  persistence:
    enabled: true
    storageClassName: longhorn
    size: 2Gi
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "500m"
  
  # Grafana configuration
  assertNoLeakedSecrets: false
  "grafana.ini":
    server:
      root_url: https://grafana.home.coredev.uk
    security:
      disable_initial_admin_creation: true
    auth:
      oauth_auto_login: true
      disable_login_form: true
      disable_signout_menu: false
    "auth.generic_oauth":
      name: PocketID
      enabled: true
      client_id: $__env{GRAFANA_OIDC_CLIENT_ID}
      client_secret: $__env{GRAFANA_OIDC_CLIENT_SECRET}
      scopes: openid profile email
      auth_url: https://pocket-id.home.coredev.uk/authorize
      token_url: http://pocket-id.pocket-id.svc.cluster.local/api/oidc/token
      api_url: http://pocket-id.pocket-id.svc.cluster.local/api/oidc/userinfo
      use_pkce: true
      email_attribute_name: email:primary
      groups_attribute_path: groups
      role_attribute_path: contains(groups[*], 'grafana_admin') && 'Admin' || contains(groups[*], 'grafana_editor') && 'Editor' || 'Viewer'
      role_attribute_strict: false
      allow_assign_grafana_admin: true
    users:
      allow_sign_up: false
  
  # Inject secret as environment variable
  envValueFrom:
    GRAFANA_OIDC_CLIENT_SECRET:
      secretKeyRef:
        name: grafana-secrets
        key: client-secret 
    GRAFANA_OIDC_CLIENT_ID:
      secretKeyRef:
        name: grafana-secrets
        key: client-id 
  
  # Grafana ingress
  ingress:
    enabled: true
    ingressClassName: traefik
    annotations:
      traefik.ingress.kubernetes.io/router.entrypoints: web,websecure
      traefik.ingress.kubernetes.io/router.tls: "true"
      traefik.ingress.kubernetes.io/redirect-to-https: "true"
      cert-manager.io/cluster-issuer: letsencrypt-prod
    hosts:
      - grafana.home.coredev.uk
    path: /
    pathType: Prefix
    tls:
      - secretName: grafana-tls
        hosts:
          - grafana.home.coredev.uk
  
  # Additional datasources
  additionalDataSources:
    - name: Loki
      type: loki
      uid: loki
      access: proxy
      url: http://monitoring-loki-gateway.monitoring.svc.cluster.local
      isDefault: false
      editable: false
      jsonData:
        maxLines: 1000
        timeout: 60
  
  # Add Glance annotations to Grafana service
  service:
    annotations:
      glance/name: "Grafana"
      glance/icon: "di:grafana"
      glance/url: "https://grafana.home.coredev.uk"
      glance/description: "Metrics Visualization"

# Alertmanager
alertmanager:
  enabled: true

# Node exporter configuration
nodeExporter:
  enabled: true

# Kube-state-metrics configuration
kubeStateMetrics:
  enabled: true

# API Server monitoring - reduce high-cardinality histogram metrics
kubeApiServer:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      # Drop high-cardinality histogram bucket metrics (saves ~30k series)
      - sourceLabels: [__name__]
        regex: 'apiserver_request_duration_seconds_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_request_body_size_bytes_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_request_sli_duration_seconds_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_response_sizes_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_watch_list_duration_seconds_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_watch_cache_read_wait_seconds_bucket'
        action: drop
      # Drop verbose request metrics we don't need for homelab
      - sourceLabels: [__name__]
        regex: 'apiserver_request_filter_duration_seconds_bucket'
        action: drop
      - sourceLabels: [__name__]
        regex: 'apiserver_admission_.*_bucket'
        action: drop
      # Drop etcd histogram buckets (exposed via apiserver)
      - sourceLabels: [__name__]
        regex: 'etcd_request_duration_seconds_bucket'
        action: drop

# etcd monitoring - reduce histogram bucket cardinality
kubeEtcd:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - sourceLabels: [__name__]
        regex: 'etcd_request_duration_seconds_bucket'
        action: drop

# Kubelet monitoring - reduce cAdvisor cardinality
kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      # Drop high-cardinality container metrics we don't need
      - sourceLabels: [__name__]
        regex: 'container_(memory_failures_total|tasks_state)'
        action: drop
    probesMetricRelabelings:
      # Drop prober histogram buckets
      - sourceLabels: [__name__]
        regex: 'prober_probe_duration_seconds_bucket'
        action: drop
    cAdvisorMetricRelabelings:
      # Drop high-cardinality cAdvisor metrics
      - sourceLabels: [__name__]
        regex: 'container_(memory_failures_total|tasks_state)'
        action: drop

# Default rules for monitoring
defaultRules:
  create: true
  rules:
    alertmanager: false
    etcd: true
    configReloaders: true
    general: true
    k8sContainerCpuUsageSecondsTotal: true
    k8sContainerMemoryCache: true
    k8sContainerMemoryRss: true
    k8sContainerMemorySwap: true
    k8sContainerResource: true
    k8sContainerMemoryWorkingSetBytes: true
    k8sPodOwner: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubeControllerManager: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeSchedulerAlerting: true
    kubeSchedulerRecording: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true
